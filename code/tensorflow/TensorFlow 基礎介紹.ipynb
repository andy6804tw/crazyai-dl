{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9a0e78-fcbc-4f64-a602-b1ae7259a001",
   "metadata": {},
   "source": [
    "# TensorFlow 基礎介紹\n",
    "\n",
    "TensorFlow 是一個由 Google 開發的機器學習平台，主要用於深度學習應用的開發和部署。它提供了豐富的工具集，從基礎層級的數學運算（如矩陣運算）到高層次的神經網絡構建，無論你是剛入門的初學者還是有經驗的研究者，TensorFlow 都是一個強大且靈活的工具。\n",
    "\n",
    "TensorFlow 能夠讓開發者方便地構建、訓練和部署深度學習模型。它被廣泛應用於影像分類、語音辨識、自然語言處理等領域，並支持多種計算平台，包括 CPU、GPU、以及 TPU，讓運算的效能能夠靈活擴展。\n",
    "\n",
    "首先，我們需要安裝並匯入相關套件。如果你還沒有安裝 TensorFlow，可以使用以下指令來安裝：\n",
    "\n",
    "```sh\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde09f5d-71a7-483a-be42-0957f3decf23",
   "metadata": {},
   "source": [
    "## 1. 張量（Tensor）\n",
    "在 TensorFlow 中，張量（Tensor）是進行所有運算的基本單位。張量的概念與 NumPy 陣列類似，但張量具備更強的擴展性，特別適合 GPU 加速的運算需求。\n",
    "\n",
    "### 1.1 張量的基本操作\n",
    "\n",
    "我們可以使用 `tf.constant()` 來創建一個靜態張量，或者使用 `tf.Variable()` 來創建一個可變張量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c90a0f-28be-47b6-baf6-edd07dc7d7c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2. 3. 4. 5.], shape=(5,), dtype=float32)\n",
      "tf.Tensor([1. 2. 3. 4. 5.], shape=(5,), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=(5,) dtype=float32, numpy=array([1., 2., 3., 4., 5.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 建立一個靜態張量\n",
    "a = tf.constant([1, 2, 3, 4, 5], dtype=tf.float32)\n",
    "print(a)\n",
    "\n",
    "# 建立一個 NumPy 陣列並轉換為張量\n",
    "b = tf.constant(np.array([1, 2, 3, 4, 5]), dtype=tf.float32)\n",
    "print(b)\n",
    "\n",
    "# 建立一個可變張量\n",
    "c = tf.Variable([1, 2, 3, 4, 5], dtype=tf.float32)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a941661-3526-464d-9b81-a5eb3c0f65da",
   "metadata": {},
   "source": [
    "TensorFlow 張量和 NumPy 陣列不同之處在於，張量可以分為兩種：一種是**無法更改值的 Tensor**，另一種是**可以更改值的 Variable**。在神經網路中，所有可訓練的變數都以 Variable 形式存在，這樣才能在訓練過程中進行數值更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67970eb9-e4c4-47d6-826e-d50c25eaa59e",
   "metadata": {},
   "source": [
    "### 1.2 張量與 NumPy 的互動\n",
    "\n",
    "張量可以輕鬆地與 NumPy 陣列相互轉換，這對於需要在 TensorFlow 與其他數值計算工具之間交互時非常有用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce00ad10-d484-4727-a600-3e41e1653671",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5.]\n",
      "tf.Tensor([5. 4. 3. 2. 1.], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 將 TensorFlow 張量轉換為 NumPy 陣列\n",
    "d = a.numpy()\n",
    "print(d)\n",
    "\n",
    "# 將 NumPy 陣列轉換為張量\n",
    "e = tf.convert_to_tensor(np.array([5, 4, 3, 2, 1]), dtype=tf.float32)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb517b4-4e48-47cd-8c92-3dfcab4f1ca2",
   "metadata": {},
   "source": [
    "### 1.3 張量的性質\n",
    "張量具有以下幾個重要的性質：\n",
    "\n",
    "1. **形狀（Shape）**：張量的形狀代表其在每個維度上的大小。形狀可以通過 `tensor.shape` 來獲取，這對於理解張量結構非常重要。\n",
    "2. **資料型別（Data Type）**：張量中的每個元素都具有相同的資料型別，例如 float32、int32 等，可以通過 `tensor.dtype` 來獲取。\n",
    "3. **設備（Device）**：張量可以被分配到不同的設備上進行計算，例如 CPU 或 GPU，可以通過 `tensor.device` 查看張量所在的設備。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab33dd3-cb8e-4a4c-8a38-ad71c2f35551",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "tensor = tf.constant([[1, 2], [3, 4], [5, 6]])\n",
    "print(tensor.shape)  # 結果為 (3, 2)，表示張量有 3 個列和 2 個行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51f5afbc-1952-438c-b2c9-57b10328138f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "tensor = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)\n",
    "print(tensor.dtype)  # 結果為 float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3afb33-1a06-459d-ab79-aec45c5d07ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    tensor = tf.constant([1, 2, 3])\n",
    "print(tensor.device)  # 顯示張量所在的設備，例如 '/job:localhost/replica:0/task:0/device:CPU:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176736fe-7fb0-4e80-a594-e41d9bbd0190",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.4 張量的數學運算\n",
    "張量支援多種數學運算，例如加減乘除、矩陣乘法、指數運算等，這些操作可以非常簡單地透過 TensorFlow 提供的函數來實現。\n",
    "\n",
    "以下是一些常見的張量運算範例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10eedfd6-f5eb-4dee-aeb4-ff7ca915f394",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 6  8]\n",
      " [10 12]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[-4 -4]\n",
      " [-4 -4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 5 12]\n",
      " [21 32]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[19 22]\n",
      " [43 50]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 建立兩個張量\n",
    "a = tf.constant([[1, 2], [3, 4]])\n",
    "b = tf.constant([[5, 6], [7, 8]])\n",
    "\n",
    "# 張量加法\n",
    "c = tf.add(a, b)\n",
    "print(c)  # 結果為 [[ 6  8]\n",
    "          #        [10 12]]\n",
    "\n",
    "# 張量減法\n",
    "d = tf.subtract(a, b)\n",
    "print(d)  # 結果為 [[-4 -4]\n",
    "          #        [-4 -4]]\n",
    "\n",
    "# 張量乘法（逐元素相乘）\n",
    "e = tf.multiply(a, b)\n",
    "print(e)  # 結果為 [[ 5 12]\n",
    "          #        [21 32]]\n",
    "\n",
    "# 矩陣乘法\n",
    "f = tf.matmul(a, b)\n",
    "print(f)  # 結果為 [[19 22]\n",
    "          #        [43 50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936605b1-631f-4988-a33b-d19ba1869c01",
   "metadata": {},
   "source": [
    "## 2. 自動計算微分值\n",
    "在深度學習演算法中，模型的權重更新是非常重要的環節，這需要對變數進行偏微分計算。\n",
    "TensorFlow 提供了一個強大的工具來進行自動微分，即 `tf.GradientTape()`。這使得神經網路的訓練過程變得非常簡單，因為你可以輕鬆地計算導數並更新模型參數。\n",
    "\n",
    "### 2.1 使用 `tf.GradientTape()` 進行微分\n",
    "以下是如何使用 `tf.GradientTape()` 來進行自動微分的例子：\n",
    "\n",
    ">若此函數 $f(x) = x^2$ 對 $x$ 做偏微分，則能得到 $f^\\prime(x) = 2*x$\n",
    ">\n",
    ">將 $x = 3$ 代入函數，得到 $f(x)=9$，$f^\\prime(x) = 6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872315a7-c565-4a04-a214-d09f2ede79da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 自動微分範例\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f(x)\n",
    "\n",
    "dy_dx = tape.gradient(y, x) \n",
    "print(dy_dx)  # 結果應該是 6.0，因為 y = x^2 對 x 的導數為 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c289f-27ec-413b-b645-622d7bb94f34",
   "metadata": {},
   "source": [
    "在上述程式碼中，我們展示了如何使用 TensorFlow 的 `tf.GradientTape()` 進行自動微分。首先，我們定義了一個變數 x，並將其初始值設定為 3.0。接著，我們定義了一個函數 f(x)，它返回 x 的平方。在 `with tf.GradientTape() as tape:` 這段程式中，我們計算了 f(x)，即 `y = x**2`。使用 `tape.gradient(y, x)`，我們計算出 y 對 x 的導數，結果為 6.0，因為 `y = x^2` 對應的導數為 `2*x`，當 x=3 時，導數的值為 6.0。\n",
    "\n",
    "這種自動微分的方式，對於計算複雜神經網路模型的梯度尤為重要，讓我們可以在訓練過程中更新權重，最小化損失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914ad14-22d4-49da-8089-3df568f24341",
   "metadata": {},
   "source": [
    "## 3. 模型建置以及訓練\n",
    "在 TensorFlow 中，我們可以通過高階 API（如 tf.keras）來快速構建和訓練模型。\n",
    "\n",
    "### 3.1 使用 Keras 來建構模型\n",
    "以下是一個簡單的線性迴歸模型的構建和訓練過程。這段程式碼展示了如何使用 tf.keras 構建並訓練一個簡單的線性迴歸模型，包括如何編譯模型、設定優化器和損失函數、提供訓練數據，以及最終進行預測。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "080e5e23-86b9-4423-9bf6-c732926eedec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 14.7342\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.5930\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0142\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9285\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7131\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0048\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5919\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3513\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2110\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1293\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0816\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0537\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0375\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0279\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0223\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0159\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0152\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0147\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0144\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0137\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0135\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0134\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0134\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0133\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0132\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0130\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0127\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0125\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0123\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0122\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0121\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0121\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0120\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0119\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0116\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0115\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0113\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0113\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0109\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0108\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0107\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0107\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0106\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0104\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0102\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0102\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0101\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "[[11.859547]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# 建立一個簡單的順序模型\n",
    "model = Sequential([\n",
    "    Dense(1, input_shape=(1,), activation='linear')\n",
    "])\n",
    "\n",
    "# 編譯模型，設定損失函數和優化器\n",
    "model.compile(optimizer=SGD(learning_rate=0.01), loss='mse')\n",
    "\n",
    "# 建立一些訓練數據\n",
    "X_train = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n",
    "y_train = np.array([2, 4, 6, 8, 10], dtype=np.float32)\n",
    "\n",
    "# 訓練模型\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# 使用模型進行預測\n",
    "print(model.predict([6]))  # 應該接近於 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f931c50-7bfe-4a91-a831-8ffe65ba3cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
