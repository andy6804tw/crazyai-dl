---
title: '1.4 分類問題'
description: 'ML Lecture 4: Classification'
keywords: machine learning ntu
tags:
    - 基礎概念
    - 李宏毅
---

# ML Lecture 4: Classification
## 分類的應用
在分類問題中我們一樣要找到一個 function。當輸入 X 特徵，模型必須輸出相對應的類別。分類的問題在現實生活中有相當多的案例，例如在金融行業上可以使用分類模型來決定是否要通過某人的貸款申請。或者是在醫療診斷上面，透過某一個病人的症狀、年紀、性別……等來預測他是罹患哪一種疾病。或者在影像識別方面可以辨識手寫數字或是中文手寫辨識。

![](https://i.imgur.com/bLCCaLk.png)

## Example Application
本篇教學以寶可夢十八個種類做分類問題做範例。

![](https://i.imgur.com/lLUXflw.png)

首先我們要將寶可夢數值化才能丟到模型裡面預測。一隻寶可夢可以用一組數字來描述他的特性，例如他的強度、生命值、攻擊力、防禦力、特殊攻擊力、特殊攻擊防禦力、速度。因此一隻寶可夢可以用一個向量來描述他，而這個向量共有七個數值作為模型輸入的特徵。我們希望能夠輸入這七著數字進入一個 function 並得到一個輸出告訴我們這是哪一種類型的寶可夢。

![](https://i.imgur.com/L15ddTo.png)

## How to do Classification
首先我們要收集所有寶可夢的資料。如果我們將分類問題用回歸的方式解的話，以二分類問題為例，輸出就是{1,−1}{1,−1}，但regression的輸出並不會剛好是1或-1 而是是一個連續性的數值。因此我們可以設定當接近 1 時就視為 class 1，接近 -1 就視為class 2。

![](https://i.imgur.com/Xqmanag.png)

但這麼做會遇到什麼問題呢? 假設我們的模型是：y=b+w1*x1+w2*x2，藍色為類別1，紅色為類別2。雖然左圖來看分佈有順利的區隔，但如右圖如果出現離群值會造成整條迴歸線受離群值影響而造成迴歸線性切割無法順利區隔。模型會為了讓離群值的輸出接近 1 而傾向那邊，近而造成整個決策邊界由綠色轉紫線。因為 Regression 對模型好壞的定義是不適合用在分類的問題。

![](https://i.imgur.com/CFwo9Hl.png)

理想上的做法是這個樣子。因為分類的模型輸出是離散的，我們可以在 function 內再加入一個 function(g) 來轉換，當 g(x)>0 的時候就當做是類別 1，否則即為類別 2。那訓練的時候 loss 該如何定義才好呢？損失函數單純統計錯誤次數，但是這樣做是無法微分的。當然他也能用其他種模型解，例如 svm、perceptron。然而在本文中將以機率的觀點來解。

![](https://i.imgur.com/zikAwSr.png)

## 機率的觀點來解
若以機率的觀點來思考。當有兩個盒子裡面各有藍綠兩個色球，去抽藍色盒子的機率為 2/3 而抽綠色盒子的機率為 1/3。那如果抽到一個藍色的球，那這球從 box1 抽的機率有多少?

![](https://i.imgur.com/62QeMER.png)

當我們把兩種箱子替換成類別一與類別二。給一個x(要分類的對象)，它屬於某個class的機率為何，我們需要知道：
- P(C1):從class1抽出來的機率
- P(C2):從class2抽出來的機率
- P(x|C1):從class1抽出x的機率
- P(x|C2):從class2抽出x的機率

有了上面四種機率，就可以計算出P(C1|x)的機率(x是class1的機率)，而這四種機率就是從我們的訓練資料去估測出來的，這種想法即為Generative Model，這可以計算某一個xx出現的機率，也就是:

> P(x)=P(x|C1)P(C1)+P(x|C2)P(C2)

上面的式子說明著，某一個x出現的機率就是它是C1的機率乘上C1挑出x的機率加上C2的機率乘上C2挑出x的機率。

![](https://i.imgur.com/XTSyjdo.png)

首先我們來計算 P(C1) 跟 P(C2) 出現的機率，此機率稱作 Prior。假設 Class1 有 79 筆 Class2 有 61 筆數據，那從 Class1 裡面抽取一隻寶可夢的機率是 `79 / (79 + 61) =0.56`，相對的 Class2 抽出來的機率是 `61 / (79 + 61) =0.44`。

![](https://i.imgur.com/GfNHFqJ.png)

我們從水系神奇寶貝中撈到海龜的機率如何計算?

![](https://i.imgur.com/0H3mrMe.png)

首先將寶可夢的特徵防禦力和特殊防禦力畫出來。從這些已有的寶可夢中想辦法估測，如果從水系的神奇寶貝挑一隻出來是海龜的機率有多少。我們可以把這79隻寶可夢的資料視為是從一個高斯分佈中挑出來的，這並不是完整的神奇寶貝的資料，只是我們剛好取到這79個點，因此我們從這個高斯分佈中取到海龜的機率並不會是0。所以我們要做的是，用這79個點來得到那一個高斯分佈。

![](https://i.imgur.com/AI29Ffo.png)

高斯分佈，可以將它視為一個function(fμ,Σ)，它的輸入是一個向量x，也就是某一個寶可夢的特徵值，輸出是x被從這個分佈中被抽取到的機率(不全然是機率)，這個機率的分佈是由μ(mean)與Σ(covariance)決定。

- 同樣的 Σ 不同的 μ 帶進同樣的 x 輸出的機率分佈最高點不一樣。
- 相同的 μ 不同的 Σ，機率分佈最高點一樣，但是分散的程度不一樣。


![](https://i.imgur.com/zUz5ZOG.png)
![](https://i.imgur.com/ZeWVUBD.png)

假設我們從Gaussian中取出79個點，現在給我們一個新的點，就可以利用Gaussian Distribution function來計算出抽到該點的機率。以分佈來看，該點愈接近中心點被抽到的機率會愈高，離中心愈遠被抽到的機率則愈低，因此黑點 New x 離中心有點遠，被抽到的機率就會有點小。

![](https://i.imgur.com/DvONaH1.png)

所以如何找出 μ 與 Σ 就是一個問題，而找出它們的概念就是 Maximum Likelihood。任何一個Gaussian都有可能找出這79個點，只是它的可能性(Likelihood)是不相同的，並且每一個點的機率都不會是0，以右上的分佈為例，離它最遠的點機率很低，但卻不會是0。上圖兩個分佈為例，左邊Gaussian找出這79個點的機率會較右邊Gaussian來的高，只要有μ與Σ我們就可以計算出這個Gaussian的Likelihood。Likelihood的計算也就是這個Gaussian抽到這79個點的機率連乘。

![](https://i.imgur.com/yi6C626.png)

因此我們要找出一個Gaussian(μ∗,Σ∗)，而這個Gaussian是找出這79點的Lieklihood是最大的。窮舉所有可能的μ,Σ，我們要得到最大的那一個，μ∗,Σ∗。

![](https://i.imgur.com/E3oVwPV.png)

有了上面的公式就可以求出兩個calss的各別μ與Σ，也就可以計算出P(C1|X)。

![](https://i.imgur.com/uIH9Xqk.png)

有了上面的公式就可以求出兩個calss的各別μ與Σ，也就可以計算出P(C1|X)。

![](https://i.imgur.com/Tdi693A.png)

上圖很清楚的看的出來，它們之間並沒有一個很明顯的決策邊界，紅色的區域是屬水系的機率較高，藍色區則是屬一般系的機率較高。而此方法在測試集的表現也普通，幾乎跟猜的是一樣的結果。

![](https://i.imgur.com/uc37mos.png)

因此我們稍微修改一下模型結構。實務上比較少每一個高斯模型都有自己的μ,Σ。這會造成參數過多的問題，而造成過擬合。常見作法是不同類別有著相同的Σ。

![](https://i.imgur.com/RIYI4OM.png)

接著讓我們調整式子，讓水系與一般系神奇寶貝擁有相同的covariance，計算它們的likelihood。上面的式子很直覺，C1就用μ1來跟Σ計算，而C2就用μ2來跟Σ計算。μ1,μ2不變，一樣是取各自類別的均值，而Σ則依各自類別總數做加權平均。

![](https://i.imgur.com/PgIL8a4.png)

調整共用covariance之後，決策邊界變為線性，這也稱為線性模型，並且考慮所有特徵之後正確率提升為73%。

![](https://i.imgur.com/kKWnXsf.png)

## 小總結
從剛剛的計算推論，我們得到機率模型三步驟：

- Model:
    - 我們有 class1、2 的機率以及產生 x 的機率分佈。有P(C1),P(C2),P(x|C1),P(x|C2) 四種機率分佈，這就是模型的參數，只要你選不同的μ(mean)與Σ(covariance)，就會得到不同的機率分佈也就會得到不同的模型

- Goodness of a function:
    - 評估這個模型的好壞就是找出一個機率分佈來最大化產生資料集的likelihood。

- Find the best function

![](https://i.imgur.com/VMRfICM.png)

不一定要使用高斯分佈，也可以使用其它的，簡單的機率模型，參數少就high bias，low variance。假設所有的特徵是獨立產生的，我們不 訓練特徵和特徵之間的 covariance 的關係。這一種方法的分類稱作 Naive Bayes Classifier。若特徵之間彼此的關聯性大的話使用簡單的模型訓練，結果會使得 bias 變大預測效果變差。

![](https://i.imgur.com/TRK05nz.png)

我們將原本的機率計算公式做一些改變。上下同除分子，分母取對數就會得到一個大家熟悉的 sigmoid 函數。

![](https://i.imgur.com/b73ST73.png)

推導到最後我們知道，P(C1|x)=σ(z)P(C1|x)=σ(z)，所以我們可以把這整個式子很簡單的寫成σ(w⋅x+b)，從這邊也可以瞭解到為什麼假設兩個類別是同一個分佈的時候，它的決策邊界會線性的。在 generative model 中，我們試著去找出N1,N2,μ1,μ2,Σ，找出這五個值，我們就可以得到w,b，也就可以得到機率。或許我們可以直接找出w,b，那就可以不用這麼麻煩的求出上述那五個值。因此在下一章中我們將來討論如何有效找出 w, b。

![](https://i.imgur.com/eKUl2VR.png)

## Reference
[簡報-Regression (Case Study)](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/Regression.pdf)

[影片-ML Lecture 4: Classification](https://youtu.be/fZAZUYEeIMg)

> 本篇文章來至於台大李宏毅教授 2017 機器學習課程[影片](https://www.youtube.com/playlist?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49)，記錄了課程重點與摘要。更多課程內容可以從[這裡](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)取得。