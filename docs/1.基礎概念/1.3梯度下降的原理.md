---
title: '1.3 æ¢¯åº¦ä¸‹é™çš„åŸç†'
description: 'ML Lecture 3-1: Gradient Descent'
keywords: machine learning ntu
tags:
    - åŸºç¤æ¦‚å¿µ
    - æå®æ¯…
---

# ML Lecture 3-1: Gradient Descent
## å›é¡§ Gradient Descent
åœ¨æ©Ÿå™¨å­¸ç¿’ç¬¬ä¸‰å€‹æ­¥é©Ÿï¼Œæˆ‘å€‘è¦æ‰¾ä¸€å€‹æœ€å¥½çš„ functionï¼Œæ˜¯è¦è§£ä¸€å€‹ optimization çš„å•é¡Œã€‚ä¹Ÿå°±æ˜¯æˆ‘å€‘åœ¨ç¬¬äºŒæ­¥æˆ‘å€‘å…ˆå®šç¾© loss functionã€‚ç›®çš„æ˜¯è¦æ‰¾ä¸€çµ„åƒæ•¸è®“é€™å€‹ loss function è¶Šå°è¶Šå¥½ã€‚æˆ‘å€‘å¯ä»¥æ¡ç”¨ Gradient Descentï¼Œå‡è¨­ç¾åœ¨ Î¸ æ˜¯ä¸€å€‹åƒæ•¸çš„é›†åˆã€‚åšæ³•æ˜¯éš¨æ©Ÿé¸ä¸€çµ„èµ·å§‹çš„åƒæ•¸å€¼ï¼Œæ¸›å» learning rate ä¹˜ä¸Š loss function å° Î¸ çš„åå¾®åˆ†ï¼Œå°±å¯ä»¥å¾—åˆ°ä¸‹ä¸€çµ„ Î¸ã€‚

![](https://i.imgur.com/09DT8kl.png)

å¦‚æœæˆ‘å€‘å°‡ Gradient Descent è¦–è¦ºåŒ–çš„è©±ä»–æœƒé•·å¾—åƒé€™æ¨£ã€‚å‡è¨­æˆ‘å€‘ç¾åœ¨æœ‰å…©å€‹åƒæ•¸ ğœƒ1 èˆ‡ ğœƒ2 ï¼Œä¸¦éš¨æ©Ÿçš„é¸ä¸€å€‹åˆå§‹ä½ç½® ğœƒ0 ã€‚æ¥ä¸‹ä¾†è¨ˆç®—åœ¨ ğœƒ0 é€™å€‹é»å®ƒçš„åƒæ•¸å° loss function çš„åå¾®åˆ†ã€‚å‡è¨­åƒæ•¸å° loss function çš„æ–œç‡æ˜¯åŒä¸­ç´…è‰²çš„ç®­é ­ã€‚æ›´æ–°åƒæ•¸çš„æ–¹å¼æ˜¯ï¼Œå°‡æ¢¯åº¦ä¹˜ä¸Šå­¸ç¿’é€Ÿç‡å†åŠ ä¸Šä¸€å€‹è² è™Ÿå°±æ˜¯åœ–ä¸­è—è‰²çš„ç®­é ­ã€‚å†å°‡è¨ˆç®—å‡ºä¾†çš„çµæœåŠ ä¸Š ğœƒ0  å°±æœƒå¾—åˆ°æ›´æ–°çš„åƒæ•¸ ğœƒ1ã€‚ä»¥ä¸Šæ­¥é©Ÿåè¦†åœ°æŒçºŒé€²è¡Œä¸‹å»ï¼Œç®—ä¸€æ¢¯åº¦æ›´æ–°ä¸€æ¬¡æ–¹å‘ç›´åˆ°æ”¶æ–‚ã€‚

![](https://i.imgur.com/wBW4AWb.png)

## Tip 1: Tuning your  learning rates
ç¬¬ä¸€å€‹è¦é»æ˜¯å°å¿ƒèª¿æ•´å­¸ç¿’é€Ÿç‡ã€‚æœ‰æ™‚å€™å­¸ç¿’é€Ÿç‡æœƒç”¢ç”Ÿä¸€äº›å•é¡Œã€‚èˆ‰ä¾‹ä¾†èªªï¼Œå‡è¨­é€™å€‹æ˜¯æˆ‘å€‘çš„ loss function çš„æ›²ç·šé•·é€™å€‹æ¨£å­ã€‚å¦‚æœä»Šå¤©å­¸ç¿’ç‡è¨­å®šçš„å‰›å‰›å¥½çš„è©±ï¼Œæœƒç…§ç´…è‰²çš„è·¯å¾‘ä¸€è·¯ä¸‹é™åˆ°æœ€ä½é»ã€‚å¦‚æœä»Šå¤©å­¸ç¿’é€Ÿç‡èª¿å¤ªå°çš„è©±ï¼Œå°±å¦‚è—è‰²çš„è·¯å¾‘æ”¶æ–‚éå¸¸æ…¢ã€‚åªè¦çµ¦ä»–å¤ å¤šçš„è¿­ä»£æ¬¡æ•¸ï¼Œä»–çµ‚ç©¶é‚„æ˜¯æœƒèµ°åˆ°è°·åº•ã€‚å¦‚æœä»Šå¤©å­¸ç¿’é€Ÿç‡èª¿æ•´æœ‰ä¸€é»å¤§ï¼Œå°±å¦‚åœ–ä¸­çš„ç¶ è‰²çš„ç®­é ­ï¼Œæ¯æ¬¡æ›´æ–°çš„æ­¥ä¼éå¤§å°è‡´æ°¸é åœ¨å±±è°·çš„å£ä¸Šé¢ä¾†å›éœ‡ç›ªè€Œç„¡æ³•èµ°åˆ°ç‰¹åˆ¥ä½çš„åœ°æ–¹ã€‚ç”šè‡³å¦‚æœå­¸ç¿’é€Ÿç‡èª¿ä¸€å€‹éå¸¸å¤§çš„è©±ï¼Œå¦‚ç¶ è‰²ç®­é ­æ‰€ç¤ºï¼Œæ›´æ–°æ­¥ä¼å¤ªå¤§æ²’è¾¦æ³•æœ‰æ•ˆæ”¶æ–‚ã€‚

![](https://i.imgur.com/WVA8rGU.png)

èª¿ä¸€å€‹å¥½çš„å­¸ç¿’é€Ÿç‡ä¸¦ä¸æ˜¯ä¸€å€‹ç°¡å–®çš„äº‹æƒ…ï¼Œæˆ‘å€‘æ¯æ¬¡å­¸ç¿’å¿…é ˆè¦å°‡æ¯å€‹è¿­ä»£çš„ loss æ›²ç·šç¹ªè£½å‡ºä¾†æ‰èƒ½è©•ä¼°æ¨¡å‹æ”¶æ–‚æƒ…å½¢ã€‚æœ‰äº›è‡ªå‹•çš„æ–¹æ³•å¯ä»¥å¹«æˆ‘å€‘èª¿æ•´å­¸ç¿’é€Ÿç‡ã€‚é€šå¸¸æˆ‘å€‘å¸Œæœ›éš¨è‘—æ›´æ–°è¿­ä»£æ¬¡æ•¸è¶Šå¤šè€Œå­¸ç¿’é€Ÿç‡æœƒé€æ¼¸è¶Šå°ã€‚å› ç‚ºç•¶æˆ‘å€‘å¾ä¸€é–‹å§‹éš¨æ©Ÿèµ·é»çš„æ™‚å€™ï¼Œæœ€å¥½çš„ä¸€çµ„è§£é€šå¸¸é›¢æœ€ä½é»æ˜¯æ¯”è¼ƒé çš„ã€‚å› æ­¤ä¸€é–‹å§‹çš„æ›´æ–°æ­¥ä¼å¸Œæœ›è¸å¤§ä¸€é»ï¼Œæ‰èƒ½æ›´å¿«æ¥è¿‘æœ€ä½é»ã€‚ä½†æ˜¯ç¶“éå¥½å¹¾æ¬¡åƒæ•¸æ›´æ–°å¾Œï¼Œå·²ç¶“é›¢ç›®æ¨™å¾ˆæ¥è¿‘äº†ï¼Œæ‰€ä»¥æ­¤æ™‚æ‡‰è©²é™ä½å­¸ç¿’é€Ÿç‡ä½¿å¾—æ¨¡å‹èƒ½å¤ æ”¶æ–‚åœ¨æœ€ä½é»çš„åœ°æ–¹ã€‚

![](https://i.imgur.com/pHpyyZw.png)

### Adagrad
Adagrad çš„æ–¹å¼æ˜¯æ¡ç”¨æ¯ä¸€å€‹å­¸ç¿’çš„åƒæ•¸éƒ½æœ‰å„è‡ªç¨ç«‹çš„å­¸ç¿’é€Ÿç‡ã€‚é€²è¡Œæ¢¯åº¦ä¸‹é™çš„æ™‚å€™ï¼Œæ¯å€‹åƒæ•¸çš„å­¸ç¿’é€Ÿç‡éƒ½æœƒé™¤ä¸Šå…ˆå‰ç®—å‡ºä¾†çš„å¾®åˆ†å€¼çš„å¹³æ–¹æ ¹ã€‚
ğœğ‘¡æ˜¯éå»æ‰€æœ‰å¾®åˆ†çš„å€¼å†åŠ ä¸Šå‡æ–¹æ ¹ï¼Œé€™ä¸€å€‹å€¼å°æ¯å€‹åƒæ•¸æ˜¯ä¸ä¸€æ¨£çš„ï¼Œå› æ­¤æ¯å€‹å­¸ç¿’é€Ÿç‡éƒ½æœƒä¸åŒã€‚

![](https://i.imgur.com/QYdk4L7.png)

é€™è£¡å¯¦éš›èˆ‰å€‹ä¾‹å­ä¾†çœ‹çœ‹ Adagrad æ˜¯å¦‚ä½•å¯¦ä½œçš„ã€‚å‡è¨­ç¾åœ¨çš„åˆå§‹å€¼æ˜¯ w0ï¼Œæ¥ä¸‹ä¾†åœ¨ w0 é‚£é»é€²è¡Œå¾®åˆ†(g0)ï¼Œè‡³æ–¼å­¸ç¿’æ•¸ç‡æ˜¯ ğœ‚0/ğœ0ã€‚å…¶ä¸­ ğœ‚ æ˜¯ä¸€å€‹ä¾è³´æ–¼æ™‚é–“åƒæ•¸ï¼Œé‚£ ğœ æœƒè¨ˆç®—éå»æ‰€æœ‰å¾®åˆ†å€¼çš„å‡æ–¹æ ¹ã€‚

![](https://i.imgur.com/PZBrTkZ.png)

ç”±æ–¼åˆ†å­åˆ†æ¯éƒ½æœ‰æ ¹è™Ÿ t+1 å› æ­¤å¯ä»¥äº’ç›¸æŠµéŠ·ã€‚

![](https://i.imgur.com/AwbRcV7.png)

Adagrad åƒæ•¸æ›´æ–°æ•´é«”è€Œè¨€æ˜¯æœƒè¶Šä¾†è¶Šæ…¢çš„ï¼Œå› ç‚ºä»–æœ‰åŠ ä¸Šæ™‚é–“ä¾è³´ã€‚è‡ªé©æ‡‰èª¿æ•´å­¸ç¿’é€Ÿç‡æœ‰å¾ˆå¤šç¨®æ–¹æ³•ï¼Œå…¶ä¸­ Adagrad åƒ…æ˜¯å…¶ä¸­ä¸€ç¨®ã€‚
Gradiant Descent çš„åƒæ•¸æ›´æ–°ï¼Œè¦çœ‹å¾®åˆ†çš„å€¼ï¼Œè¶Šå¤§æ›´æ–°è¶Šå¿«ã€‚åœ¨ Adagrad çš„å…¬å¼è£¡é¢ï¼Œåˆ†å­ç´…è‰²éƒ¨åˆ†æ˜¯å¾®åˆ†è¶Šå¤§ï¼Œåƒæ•¸ update çš„æ­¥ä¼è¶Šå¤§ã€‚ä½†æ˜¯åˆ†æ¯è—è‰²éƒ¨åˆ†çš„æ•ˆæœï¼Œå½±éŸ¿ç•¶å¾®åˆ†è¶Šå¤§ï¼Œåƒæ•¸ Â update æ­¥ä¼è¶Šå°ï¼Œè·Ÿåˆ†å­çš„å½±éŸ¿è¡çªã€‚

![](https://i.imgur.com/jw9nKiE.png)

å› æ­¤æœ‰é€™éº¼ä¸€å€‹èªªæ³•ã€‚Adagrad æ‰€è¡¨ç¤ºçš„æ˜¯æ¢¯åº¦çš„åå·®ã€‚å‡è¨­æœ‰å€‹åƒæ•¸ï¼Œä»–ç®—åˆ°å…¶ä¸­ä¸€å€‹é»ç®—å‡ºä¾†ç‰¹åˆ¥å¤§æˆ–æ˜¯ç®—åˆ°æŸä¸€æ¬¡å‡ºç¾ç‰¹åˆ¥å°çš„å€¼ adagrad åˆ©ç”¨éå» gradiant çš„å¹³å‡ä¾†çœ‹åå·®çš„æ•ˆæœã€‚

![](https://i.imgur.com/6wLpfGG.png)

## Tip 2: Stochastic  Gradient Descent
æˆ‘å€‘å¯ä»¥ä½¿ç”¨ Stochastic Gradient Descent ä½¿æˆ‘å€‘è¨“ç·´é€Ÿåº¦è®Šå¿«ã€‚ä¸€èˆ¬çš„æ¢¯åº¦ä¸‹é™ loss function æœƒè€ƒæ…®æ‰€æœ‰è³‡æ–™é›†ï¼Œå†ä»¥æ‰€æœ‰è³‡æ–™é›†çš„ç¸½èª¤å·®ä¾†è¨ˆç®—æ¢¯åº¦ä¸‹é™ï¼Œä½† Stochastic Gradient Descent(éš¨æ©Ÿæ¢¯åº¦)åªè€ƒæ…®ä¸€ç­†è³‡æ–™èª¤å·®ï¼Œæ¢¯åº¦ä¹Ÿåªè€ƒæ…®è©²ç­†è³‡æ–™ã€‚ä¹Ÿå°±æ˜¯æ¯çœ‹å®Œä¸€ç­†è³‡æ–™å°±æ›´æ–°ä¸€æ¬¡åƒæ•¸ã€‚

![](https://i.imgur.com/Xm3Fmtj.png)


éš¨æ©Ÿæ¢¯åº¦ä¸‹é™èˆ‡æ¢¯åº¦ä¸‹é™çš„æœ€å¤§å·®ç•°åœ¨æ–¼ï¼Œæ¢¯åº¦ä¸‹é™æ¯æ¬¡çš„è¿­ä»£æ›´æ–°éƒ½æœƒè¨ˆç®—ä¸€æ¬¡æ‰€æœ‰çš„è³‡æ–™èª¤å·®å†åšæ¢¯åº¦ä¸‹é™ï¼Œè€Œéš¨æ©Ÿæ¢¯åº¦ä¸‹é™å‰‡æ˜¯æ¯æ¬¡çš„è¿­ä»£éƒ½åªè¨ˆç®—ä¸€ç­†çš„èª¤å·®ä¸¦ä¸”æ›´æ–°ã€‚å› æ­¤å¯ä»¥ç™¼ç¾éš¨æ©Ÿæ¢¯åº¦çš„æ”¶æ–‚ç„¡æ³•åƒæ¢¯åº¦ä¸‹é™ä¸€æ¨£å¾ˆç©©å®šçš„å¾€æœ€ä½³è§£å‰é€²ï¼Œå®ƒçš„æ±‚è§£éç¨‹ä¸­è¼ƒç‚ºéœ‡ç›ªã€‚

![](https://i.imgur.com/ZVmrIXG.png)

## Tip 3: Feature Scaling
å‡è¨­ä¸€å€‹è¿´æ­¸çš„æ¨¡å‹æœ‰å…©å€‹ç‰¹å¾µ x1  å’Œ  x2 Â è‹¥é€™å…©å€‹ç‰¹å¾µçš„åˆ†å¸ƒå¾ˆä¸ä¸€æ¨£ï¼Œé‚£æˆ‘å€‘è¦é€é scaling è®“ä»–å€‘å…©å€‹åˆ†å¸ƒä¸€è‡´ã€‚å¾ä¸Šåœ–æˆ‘å€‘å¯ä»¥çŸ¥é“ X2 çš„åˆ†ä½ˆé æ¯” X1 å¤§ï¼Œå»ºè­°æœ€å¥½å°‡ x2 é€²è¡Œç‰¹å¾µç¸®æ”¾è®“ x2 èˆ‡ x1 çš„åˆ†ä½ˆæ˜¯ä¸€è‡´çš„ã€‚

![](https://i.imgur.com/K0caqtI.png)

æˆ‘å€‘ç‚ºä»€éº¼å¸Œæœ›ä¸åŒçš„ç‰¹å¾µå®ƒå€‘çš„ scale æ˜¯ä¸€æ¨£çš„å‘¢ï¼Ÿä»¥ä¸‹èˆ‰å€‹ç°¡å–®ä¾‹å­ï¼Œå‡è¨­æœ‰ä¸€å€‹è¿´æ­¸çš„ functionã€‚è‹¥ä¸åš feature scaling çš„æƒ…æ³ä¸‹ï¼Œå¦‚æœç‰¹å¾µé–“çš„å·®ç•°éå¤§ï¼Œæœƒå‘ˆç¾æ©¢åœ“å‹ã€‚å› ç‚º w1 å°æ–¼ loss çš„å½±éŸ¿æ¯”è¼ƒå°è€Œ w2 å°æ–¼ loss çš„å½±éŸ¿æ¯”è¼ƒå¤§ ã€‚åœ¨ç¶“éç¸®æ”¾ä¹‹å¾Œæœƒä»–å€‘çš„å½±éŸ¿é—œä¿‚æ˜¯å‘ˆç¾æ­£åœ“ï¼Œå› æ­¤æ”¶æ–‚æ–¹å‘å¯ä»¥å¾ˆæ˜ç¢ºåœ°å¾€åœ“å¿ƒèµ°è®“æ¢¯åº¦ä¸‹é™æ›´æœ‰æ•ˆç‡ã€‚

![](https://i.imgur.com/0Hna6Xy.png)

ä»¥ä¸€èˆ¬çš„æ¨™æº–æ­£è¦åŒ–ä¾†èªªï¼Œæˆ‘å€‘å¯ä»¥è¨ˆç®—æ¯å€‹ç‰¹å¾µåœ¨è³‡æ–™é›†ä¸­çš„å¹³å‡å€¼èˆ‡è®Šç•°æ•¸ï¼Œå°‡æ‰€æœ‰ç‰¹å¾µç¸®æ”¾ç‚ºå‡å€¼ç‚º0ï¼Œæ–¹å·®ç‚º1ã€‚

![](https://i.imgur.com/hFfEGkP.png)

## Reference
[ç°¡å ±-Gradient Descent](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Gradient%20Descent%20(v2).pdf)

[å½±ç‰‡-ML Lecture 3-1: Gradient Descent](https://www.youtube.com/watch?v=yKKNr-QKz2Q&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=7)

> æœ¬ç¯‡æ–‡ç« ä¾†è‡³æ–¼å°å¤§æå®æ¯…æ•™æˆ2017 æ©Ÿå™¨å­¸ç¿’èª²ç¨‹[å½±ç‰‡](https://www.youtube.com/playlist?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49)ï¼Œè¨˜éŒ„äº†èª²ç¨‹é‡é»èˆ‡æ‘˜è¦ã€‚æ›´å¤šèª²ç¨‹å…§å®¹å¯ä»¥å¾[é€™è£¡](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html)å–å¾—ã€‚