



<!doctype html>
<html lang="zh-Hant" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://andy6804tw.github.io/crazyai-dl/5.Transformer架構/5.1Attention Is All You Need/">
      
      
        <meta name="author" content="10程式中">
      
      
        <meta name="lang:clipboard.copy" content="複製">
      
        <meta name="lang:clipboard.copied" content="已複製">
      
        <meta name="lang:search.language" content="ja">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="沒有符合的項目">
      
        <meta name="lang:search.result.one" content="找到 1 個符合的項目">
      
        <meta name="lang:search.result.other" content="找到 # 個符合的項目">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.4.0">
    
    
      
        <title>5.1 Transformer — Attention Is All You Need</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.0284f74d.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.01803549.css">
      
      
        
        
        <meta name="theme-color" content="#ef5350">
      
    
    
      <script src="../../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="red" data-md-color-accent="red">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#_1" tabindex="1" class="md-skip">
        跳轉到
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://andy6804tw.github.io/crazyai-dl/" title="全民瘋AI系列 [深度學習與神經網路]" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              全民瘋AI系列 [深度學習與神經網路]
            </span>
            <span class="md-header-nav__topic">
              
                5.1 Transformer — Attention Is All You Need
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜尋" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            打字進行搜尋
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/andy6804tw/crazyai-dl" title="前往倉庫" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    GitHub
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://andy6804tw.github.io/crazyai-dl/" title="全民瘋AI系列 [深度學習與神經網路]" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    全民瘋AI系列 [深度學習與神經網路]
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/andy6804tw/crazyai-dl" title="前往倉庫" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      1.基礎概念
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        1.基礎概念
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../1.1迴歸分析/" title="1.1 迴歸分析" class="md-nav__link">
      1.1 迴歸分析
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../1.2探索誤差的來源/" title="1.2 探索誤差的來源" class="md-nav__link">
      1.2 探索誤差的來源
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../1.3梯度下降的原理/" title="1.3 梯度下降法：優化的核心" class="md-nav__link">
      1.3 梯度下降法：優化的核心
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../1.4分類問題/" title="1.4 分類問題" class="md-nav__link">
      1.4 分類問題
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../1.5邏輯迴歸/" title="1.5 邏輯迴歸" class="md-nav__link">
      1.5 邏輯迴歸
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      2.深度神經網路
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        2.深度神經網路
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../2.1深度學習簡介/" title="2.1 深度學習簡介" class="md-nav__link">
      2.1 深度學習簡介
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      3. 卷積神經網路
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        3. 卷積神經網路
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../3.卷積神經網路/3.1CNN/" title="3.1CNN" class="md-nav__link">
      3.1CNN
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../3.卷積神經網路/3.2ResNet/" title="3.2 ResNet 殘差神經網路" class="md-nav__link">
      3.2 ResNet 殘差神經網路
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      5.Transformer架構
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        5.Transformer架構
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        5.1 Transformer — Attention Is All You Need
      </label>
    
    <a href="./" title="5.1 Transformer — Attention Is All You Need" class="md-nav__link md-nav__link--active">
      5.1 Transformer — Attention Is All You Need
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">本頁目錄</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" title="前言" class="md-nav__link">
    前言
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-rnn" title="重新審視 Attention + RNN" class="md-nav__link">
    重新審視 Attention + RNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-for-seq2seq-model" title="Attention for Seq2Seq Model" class="md-nav__link">
    Attention for Seq2Seq Model
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn-attention-qkv" title="RNN 的 Attention Ｑ、K、V 計算" class="md-nav__link">
    RNN 的 Attention Ｑ、K、V 計算
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention-for-seq2seq-model_1" title="實際範例 Attention for Seq2Seq Model" class="md-nav__link">
    實際範例 Attention for Seq2Seq Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-without-rnn" title="Attention without RNN" class="md-nav__link">
    Attention without RNN
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attention-layer" title="Attention Layer" class="md-nav__link">
    Attention Layer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#self-attention-without-rnn" title="Self-Attention without RNN" class="md-nav__link">
    Self-Attention without RNN
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-attention-layer" title="Self-Attention Layer" class="md-nav__link">
    Self-Attention Layer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" title="小結" class="md-nav__link">
    小結
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" title="Reference" class="md-nav__link">
    Reference
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../5.2Transformer/" title="5.2 Transformer 機制解說 (上)" class="md-nav__link">
      5.2 Transformer 機制解說 (上)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../5.3Transformer/" title="5.3 Transformer 機制解說 (下)" class="md-nav__link">
      5.3 Transformer 機制解說 (下)
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">本頁目錄</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" title="前言" class="md-nav__link">
    前言
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-rnn" title="重新審視 Attention + RNN" class="md-nav__link">
    重新審視 Attention + RNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-for-seq2seq-model" title="Attention for Seq2Seq Model" class="md-nav__link">
    Attention for Seq2Seq Model
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn-attention-qkv" title="RNN 的 Attention Ｑ、K、V 計算" class="md-nav__link">
    RNN 的 Attention Ｑ、K、V 計算
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention-for-seq2seq-model_1" title="實際範例 Attention for Seq2Seq Model" class="md-nav__link">
    實際範例 Attention for Seq2Seq Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-without-rnn" title="Attention without RNN" class="md-nav__link">
    Attention without RNN
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attention-layer" title="Attention Layer" class="md-nav__link">
    Attention Layer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#self-attention-without-rnn" title="Self-Attention without RNN" class="md-nav__link">
    Self-Attention without RNN
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-attention-layer" title="Self-Attention Layer" class="md-nav__link">
    Self-Attention Layer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" title="小結" class="md-nav__link">
    小結
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" title="Reference" class="md-nav__link">
    Reference
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>5.1 Transformer — Attention Is All You Need</h1>
                
                <h2 id="_1">前言</h2>
<p>Transformer 完全基於 Attention 注意力機制的架構。Attention 原先是被應用在 RNN，之後 Google 所提出的 Transformer 保留了原先 Attention 的優勢並移除了 RNN 的架構。Transformer 是一個蠻新的模型，最先由 2017 年被 Google 所提出一篇叫 Attention is all you need 的論文。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-1.png" /></p>
<p>Transformer 是一種 Seq2seq 的模型，他有一個 Encoder 和 Decoder 並且非常適合做機器翻譯。另外在 Transformer 中拋棄了 RNN 循環神經網路的架構。Transformer 僅保留了 Attention 機制以及全連接層網路，實驗結果並優於 RNN+Attemtion 的架構。目前最新的機器翻譯研究已經很少人用 RNN 模型了，當今業界大多使用 Transformer + Bert 模型。</p>
<ul>
<li>Transformer is a Seq2Seq model.</li>
<li>Transformer is not RNN.</li>
<li>Purely based attention and dense layers.</li>
<li>Higher accuracy than RNNs on large  datasets.</li>
</ul>
<h2 id="attention-rnn">重新審視 Attention + RNN</h2>
<p>這裡來思考一個問題。當我們把 RNN 去掉只保留 Attention，僅利用 Attention 搭建一個神經網路用來取代 RNN。那我們該怎麼做呢？接下來我們會來詳細討論，從零開始基於 Attention 搭建一個神經網路的整個流程。首先在本篇文章我們先將之前學過的 RNN + Attention 開始入手，再抽取掉 RNN 保留 Attention。然後搭建一個 Attention 與 Self-Attention 網路層。下一篇文章會再將這些概念組裝起來，搭建一個深度的 Seq2seq 模型。搭出來的模型就是當今最紅的 Transformer。</p>
<h2 id="attention-for-seq2seq-model">Attention for Seq2Seq Model</h2>
<h3 id="rnn-attention-qkv">RNN 的 Attention Ｑ、K、V 計算</h3>
<p>在<a href="https://andy6804tw.github.io/2021/05/01/rnn-to-attention/">前篇</a>文章有提到 RNN 模型的進化，最終使用 Attention 機制來改善 RNN Seq2seq 的模型。所謂的 Seq2seq 是指有一個 Encoder 和一個 Decoder。Encoder 的輸入是有 m 個時間點的輸入X<sub>1</sub>~X<sub>m</sub>，每個一個輸入都是經過編碼過後的向量。Encoder 把這些輸入的訊息壓縮到隱藏狀態向量 h 中，其最後一個狀態 h<sub>m</sub> 是看過所有的輸入後所壓縮的訊息。Decoder 所做的事情取決於你的任務是什麼，例如文字生成器。在 Decoder 中會依序產生出狀態 s，每個時間點會根據狀態生成一個文字。我們在把輸出的文字作為下一次的輸入x<sup>‘</sup>，如果有 Attention 機制的話還需要計算 context vector(c)。每當計算出一個狀態 s 就要計算一次 c。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-2.png" /></p>
<p>context vector(c) 計算方式是，首先將 Decoder 當前狀態 s<sub>j</sub> 與 Encoder 所有狀態 h<sub>1</sub>~h<sub>m</sub> 做對比並用 align() 函數計算彼此間的相關性。把算出的 𝛼<sub>ij</sub>  作為注意力的分數。 每計算一次 context vector 就要計算出 m 個分數，其表示 𝛼<sub>1j</sub> ~ 𝛼<sub>mj</sub> 。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-3.png" /></p>
<p>每一個 𝛼 對應一個狀態 h，以下我們具體的來看一下分數 𝛼 是如何被計算出來的。分數的計算是 h<sub>i</sub>  和 s<sub>j</sub> 的函數，首先我們必須計算 Q 和 K。把向量 h<sub>i</sub> 乘上一個矩陣 W<sub>K</sub> 得到 k<sub>i</sub>。 另外把向量 s<sub>j</sub> 乘上一個矩陣 W<sub>Q</sub> 得到 q<sub>j</sub>。這裡的矩陣 W<sub>K</sub> 與 W<sub>Q</sub> 是 align() 函數中可以學習的權重，必須經由訓練資料中去學習的。我們必須把 s<sub>j</sub> 這一個向量與 Encoder 中的所有 h 去計算對比。有 m 個 h 向量因此會有 m 個 k，我們可以將 k<sub>1</sub>~k<sub>m</sub> 組成一個 K 矩陣。我們可以發現圖中綠色的 k<sub>i</sub> 向量為 K 的每一行(col)。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-4.png" /></p>
<p>計算每個分數需要將 K 進行轉置並與 q<sub>j</sub> 進行矩陣相乘的運算。輸出會是一個 m 維的向量。最後再使用一個 Softmax 函數將這些輸出的數值映射到 0~1 之間，並且這 m 個數值加總必為 1。此時的 𝛼<sub>1j</sub>~ 𝛼<sub>mj</sub> 為最終 q<sub>j</sub>  所對輸入有感興趣的地方的分數。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-5.png" /></p>
<p>剛才已經將 Decoder 狀態 s<sub>j</sub> 還有 Encoder 狀態 h<sub>i</sub> 分別做線性轉換，得到一組向量 q<sub>j</sub>(Query) 與 m 個 k<sub>i</sub>(Key)。我們拿一個 q<sub>j</sub> 向量去對比所有 Key(K)，算出 m 個分數，這 m 個 𝛼 分數表示了 Query 與每一個 Key 的匹配程度。其匹配程度越高 𝛼 分數越大，同時也代表著模型需要更關注這些內容。除此之外我們還需要計算 Value，將 h<sub>i</sub> 乘上一個矩陣 W<sub>V</sub> 上 就能得到 v<sub>1</sub>~v<sub>m</sub>。這些合併起來就能用 V 表示，另外這裡的 W<sub>V</sub> 也是可以透過機器學習的。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-6.png" /></p>
<h3 id="attention-for-seq2seq-model_1">實際範例 Attention for Seq2Seq Model</h3>
<p>剛剛已經講了 Q、K、V 這三種向量 在 RNN 架構中是如何被計算出來的。我們再回過頭看一下這個例子。首先我們先把 Decoder 目前狀態 s<sub>j</sub> 乘上一個 W<sub>Q</sub> 得到 q<sub>j</sub>。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-7.png" /></p>
<p>然後把 Encoder 所有 m 個狀態 h<sub>1</sub>~h<sub>m</sub> 乘上 W<sub>K</sub> 映射到 Key 向量。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-8.png" /></p>
<p>用矩陣 K 與向量 q<sub>j</sub> 計算出 m 維的分數向量。a<sub>1j</sub>~a<sub>mj</sub> 對應每個 Encoder 的 h 向量。最後還要經過一個 Softmax() 即代表對輸入 x<sub>1</sub>~x<sub>m</sub> 所需要關注的分數。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-9.png" /></p>
<p>接下來計算 Value 向量 v<sub>i</sub>，我們拿 Encoder 第 i 個狀態向量 h<sub>i</sub> 與一個權重 W<sub>V</sub> 做一個線性轉換得到 v<sub>i</sub>。每一個 v<sub>i</sub> 對應一個隱藏狀態 h。最終我們將會得到 m 個 𝛂 與 v，並做加權平均得到一組新的 context vector(c)。c<sub>j</sub> 等於 𝛼<sub>1j</sub> 乘上 v<sub>1</sub> 一直加到 𝛼<sub>mj</sub> 乘上 v<sub>m</sub>。 這種計算分數 𝛼 和 context vector(c) 的方法就是 Transformer 用的機制。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-10.png" /></p>
<h2 id="attention-without-rnn">Attention without RNN</h2>
<p>這一個部分我們來討論捨棄 RNN 只保留 Attention 的 Transformer，並得到一個 Attention 與 Self-Attention Layer。</p>
<h3 id="attention-layer">Attention Layer</h3>
<p>首先我們先設計一個 Attention Layer 用於 Seq2seq 模型，一樣包含一個 Encoder 與一個 Decoder。Encoder 的輸入向量是 x<sub>1</sub>~x<sub>m</sub>。Decoder 的輸入是 x’1~x’t。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-11.png" /></p>
<p>這裡我們捨去 RNN 只用 Attention。首先拿 Encoder 的輸入 x<sub>1</sub>~x<sub>m</sub> 來計算 Key 與 Value 向量。於是 x<sub>1</sub> 就被映射成 k<sub>1</sub> 與 v<sub>1</sub>，x2 就被映射成 k<sub>2</sub> 與 v<sub>2</sub>，依此類推我們就得到 m 組的 k 和 v 向量。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-12.png" /></p>
<p>然後把 Decoder 的輸入 x’<sub>1</sub>~x’<sub>t</sub> 做一個線性轉換乘上 Wq 得到 Query。若 Decoder 有 t 個輸入向量，則將會有 t 個 query q<sub>1</sub>~q<sub>t</sub>。注意一下目前為止總共出現了三個 W 矩陣，分別為 Encoder 中的 W<sub>K</sub> 和 W<sub>V</sub> 與 Decoder 中的 W<sub>Q</sub>。這些權重都是可以經由訓練資料進行學習的權重。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-13.png" /></p>
<p>現在開始計算分數𝛂，拿取 Decoder 中的第一個 q<sub>1</sub> 與所有 Encoder 中 m 個 k 向量做對比。透過 Scaled dot product 計算出每一個輸入的分數，也就是所謂的 Attention 關聯強度。我們將會得到 m 維的向量 𝛂1(𝛂<sub>11</sub>~𝛂<sub>m1</sub>)，裏面代表著每個相對應輸入的注意力程度。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-14.png" /></p>
<p>然後再計算 context vector c<sub>1</sub>，需要用到分數向量 𝜶<sub>1</sub> 與所有 m 個 value 向量進行加權和。又可以寫成 V𝜶<sub>1</sub>。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-15.png" /></p>
<p>接下來重複上述步驟可以得到所有 context vector，每一個 c 對應一個 x’。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-16.png" /></p>
<p>假設有七個輸出將會有七個 context vector，分別為 c<sub>1</sub>~c<sub>7</sub> 為最終的輸出。並且可以用 C 表示這些向量。想要計算一個向量 c<sub>j</sub> 要用到所有的 Q、K、V。所以 c<sub>2</sub> 依賴於 x’<sub>2</sub> 以及 Encoder 中所有的輸入，並透過注意力分數來取捨資訊。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-17.png" /></p>
<p>我們把 Attention layer 稱之為函數 Attn()，輸入是分別是 Encoder 的 x<sub>1</sub>~x<sub>m</sub> (X)以及 Decoder 的 x’<sub>1</sub>~x’<sub>t</sub> (X’)。除此之外 Attention layer 有三個要學習的權重矩陣 W<sub>Q</sub>、W<sub>K</sub>、W<sub>V</sub>。最後 Attention layer 的輸出是 c<sub>1</sub>~c<sub>t</sub> (C) 共 t 個向量。因此我們可以總結 Attention layer 有兩個輸入 X 與 X‘，以及一個輸出 C，每一個 c 向量對應一個 x’ 向量。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-18.png" /></p>
<h2 id="self-attention-without-rnn">Self-Attention without RNN</h2>
<p>剛才我們將 Seq2seq 模型剔除了 RNN，並採用 Attention Layer 替代。接下來要來了解 Self-Attention Layer。基本上原理完全一模一樣，我們可以使用 Self-Attention 來取代 RNN。</p>
<h3 id="self-attention-layer">Self-Attention Layer</h3>
<p>Self-Attention Layer 並非 Seq2seq 它僅有一個輸入序列，同時可以使用 Attn() 函數表示。此函數跟先前提的方法一模一樣，差別在於函數的輸入都是 X。此外輸出的序列是 c<sub>1</sub>~c<sub>m</sub> 與輸入 x 的虛列長度是一樣的都是 m，每一個 c 向量都對應一個 x 向量。但是必須注意，舉例來說 c<sub>2</sub> 並非只依賴於 x<sub>2</sub>，他是依賴於 x<sub>1</sub>~x<sub>m</sub> 也就是每個輸入都會考慮過才算出 c<sub>2</sub>。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-19.png" /></p>
<p>Self-Attention Layer 的原理跟 Attention Layer 完全一模一樣。只差別於輸入不同，在 Self-Attention 中僅有一個輸入序列 x<sub>1</sub>~x<sub>m</sub>。第一步是做三種轉換將 x<sub>i</sub> 映射到 q<sub>i</sub>、k<sub>i</sub>、v<sub>i</sub> 並得到三個向量。權重矩陣依然是 W<sub>Q</sub>、W<sub>K</sub>、W<sub>V</sub> 對輸入 x 做線性轉換。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-20.png" /></p>
<p>每個 x 輸入都做了線性轉換後會得到 q、k、v 三個向量。接下來再計算分數向量 𝛼 ，公式還是一樣的。我們將矩陣 K(k<sub>1</sub>~k<sub>m</sub>) 轉置乘上 q<sub>j</sub> 向量然後做 Softmax 得到 m 維向量 𝛼<sub>j</sub>。我們可以從圖中的例子看到 𝛼<sub>1</sub> 依賴於 q<sub>1</sub> 以及所有 k 向量 k<sub>1</sub>~k<sub>m</sub>。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-21.png" /></p>
<p>依此類推 𝛼<sub>2</sub> 依賴於 q<sub>2</sub> 以及所有 k 向量 k<sub>1</sub>~k<sub>m</sub>。用同樣的公式計算出所有分數向量 𝛼。總共有 m 個 𝛼 向量，此外每個分數向量都是 m 維的。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-22.png" /></p>
<p>現在可以開始計算 context vector。 c<sub>1</sub> 是所有 m  個 v 向量與 𝛼 的加權和。看以下這張圖，c<sub>1</sub> 依賴於分數向量 𝛼<sub>1</sub>，以及所有 m 個 v 向量 v<sub>1</sub>~v<sub>m</sub>。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-23.png" /></p>
<p>計算同樣的步驟算出 m 個 c 向量得到 c<sub>1</sub>~c<sub>m</sub>。這 m 個 c 向量就是 Self-Attention Layer 的輸出。其中第 j 個輸出 c<sub>j</sub> 是依賴於矩陣 V、K 以及向量 q<sub>j</sub>。
因為所有的 c<sub>j</sub> 依賴於所有的 K 與 V，所以 c<sub>j</sub> 依賴於所有 m 個 x 向量 x<sub>1</sub>~x<sub>m</sub>。下圖中每個輸入 x<sub>i</sub> 位置上都對應一個輸出 c<sub>i</sub>，每個 c<sub>i</sub> 並非只關注自己的 x<sub>i</sub> 而是依賴於所有的 x。只要改變任何一個 x 所有的 c<sub>i</sub> 都會發生變化。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-24.png" /></p>
<p>我們已經學習了 Self-Attention Layer 的運作機制。輸入是一個序列 x<sub>1</sub>~x<sub>m</sub>，這一個網路曾將有三組權重矩陣分別有 W<sub>Q</sub>、W<sub>K</sub>、W<sub>V</sub>。這三個矩陣能把每個 x 映射到 q、k、v 三個向量。其每一個輸出也是一個序列 c<sub>1</sub>~c<sub>m</sub> 共有 m 個向量，每一個 x 位置上都有對應的 c。
Attention 與 Self-Attention 都用 Attn() 這個函數來表示，此函數有兩個輸入矩陣。Attention Layer 的輸入是 X 與 X’ 而 Self-Attention 的輸入是兩個相同的 X。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-25.png" /></p>
<h2 id="_2">小結</h2>
<p>到目前為止已經說明了 Attention 與 Self-Attention Layer，最後做一個小結。Attention 的想法最初在 2015 年由 Bengio 實驗室所發表的論文中。此篇論文使用 Attention 改進 Seq2seq 模型，後來大家發現 Attention 並不局限於 Seq2seq 模型，而是可以使用在所有的 RNN 上。如果僅有一個 RNN 網路，那麼 Attention 就稱為 Self-Attention。Self-Attention 這篇論文於 2016 年被發表，再後來 Google 於 2017 年發表的 Attention Is All You Need 表示根本不需要使用到 RNN。直接單獨使用 Attention 效果會更好。另外此篇論文中提出了 Transformer 模型架構，也就是下篇文章將提到的部分。</p>
<blockquote>
<p>本篇文章內容來至於線上課程 <a href="https://github.com/wangshusen/DeepLearning">CS583: Deep Learning</a></p>
</blockquote>
<h2 id="reference">Reference</h2>
<p>[1] Bahdanau, Cho, &amp; Bengio. Neural machine translation by jointly learning to align and  translate. In ICLR, 2015.</p>
<p>[2] Cheng, Dong, &amp; Lapata. Long Short-Term Memory-Networks for Machine Reading. In
EMNLP, 2016.</p>
<p>[3] Vaswani et al. Attention Is All You Need. In NIPS, 2017.</p>
<p><a href="https://www.youtube.com/watch?v=aButdUV0dxI">Transformer模型(1/2): 剝離RNN，保留Attention</a></p>
<p><a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/10_Transformer_1.pdf">簡報</a></p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../../3.卷積神經網路/3.2ResNet/" title="3.2 ResNet 殘差神經網路" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  上一頁
                </span>
                3.2 ResNet 殘差神經網路
              </span>
            </div>
          </a>
        
        
          <a href="../5.2Transformer/" title="5.2 Transformer 機制解說 (上)" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  下一頁
                </span>
                5.2 Transformer 機制解說 (上)
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright © 2023 - 2024 10程式中
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.245445c6.js"></script>
      
        
        
          
          <script src="../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../../assets/javascripts/lunr/lunr.ja.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="../../javascripts/extra.js"></script>
      
    
  </body>
</html>