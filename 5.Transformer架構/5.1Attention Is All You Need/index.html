
<!doctype html>
<html lang="zh-TW" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="10程式中">
      
      
        <link rel="canonical" href="https://andy6804tw.github.io/crazyai-dl/5.Transformer%E6%9E%B6%E6%A7%8B/5.1Attention%20Is%20All%20You%20Need/">
      
      
        <link rel="prev" href="../../3.%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/3.2ResNet/">
      
      
        <link rel="next" href="../5.2Transformer/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.42">
    
    
      
        <title>5.1 Transformer — Attention Is All You Need - 全民瘋AI系列 [深度學習與神經網路]</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="red">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳轉到
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="頁首">
    <a href="../.." title="全民瘋AI系列 [深度學習與神經網路]" class="md-header__button md-logo" aria-label="全民瘋AI系列 [深度學習與神經網路]" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            全民瘋AI系列 [深度學習與神經網路]
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              5.1 Transformer — Attention Is All You Need
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜尋" placeholder="搜尋" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="搜尋">
        
        <button type="reset" class="md-search__icon md-icon" title="清除" aria-label="清除" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜尋引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/andy6804tw/crazyai-dl" title="前往倉庫" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="導覽列" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="全民瘋AI系列 [深度學習與神經網路]" class="md-nav__button md-logo" aria-label="全民瘋AI系列 [深度學習與神經網路]" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    全民瘋AI系列 [深度學習與神經網路]
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/andy6804tw/crazyai-dl" title="前往倉庫" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    1.基礎概念
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            1.基礎概念
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5/1.1%E8%BF%B4%E6%AD%B8%E5%88%86%E6%9E%90/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.1 迴歸分析
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5/1.2%E6%8E%A2%E7%B4%A2%E8%AA%A4%E5%B7%AE%E7%9A%84%E4%BE%86%E6%BA%90/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.2 探索誤差的來源
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5/1.3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%8E%9F%E7%90%86/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.3 梯度下降的原理
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5/1.4%E5%88%86%E9%A1%9E%E5%95%8F%E9%A1%8C/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.4 分類問題
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1.%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5/1.5%E9%82%8F%E8%BC%AF%E8%BF%B4%E6%AD%B8/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.5 邏輯迴歸
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    2.深度神經網路
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            2.深度神經網路
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2.%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/2.1%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E7%B0%A1%E4%BB%8B/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.1 深度學習簡介
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    3. 卷積神經網路
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            3. 卷積神經網路
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../3.%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/3.1CNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.1CNN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../3.%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/3.2ResNet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.2 ResNet 殘差神經網路
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    5.Transformer架構
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            5.Transformer架構
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    5.1 Transformer — Attention Is All You Need
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    5.1 Transformer — Attention Is All You Need
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目錄">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目錄
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      前言
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      重新審視 Attention + RNN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-for-seq2seq-model" class="md-nav__link">
    <span class="md-ellipsis">
      Attention for Seq2Seq Model
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention for Seq2Seq Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn-attention-qkv" class="md-nav__link">
    <span class="md-ellipsis">
      RNN 的 Attention Ｑ、K、V 計算
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention-for-seq2seq-model_1" class="md-nav__link">
    <span class="md-ellipsis">
      實際範例 Attention for Seq2Seq Model
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-without-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      Attention without RNN
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention without RNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attention-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Attention Layer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#self-attention-without-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Attention without RNN
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Self-Attention without RNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-attention-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Attention Layer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      小結
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    <span class="md-ellipsis">
      Reference
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5.2Transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5.2 Transformer 機制解說 (上)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5.3Transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5.3 Transformer 機制解說 (下)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    TensorFlow 101 從零開始
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            TensorFlow 101 從零開始
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../TensorFlow%20101%20%E5%BE%9E%E9%9B%B6%E9%96%8B%E5%A7%8B/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    系列介紹
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../TensorFlow%20101%20%E5%BE%9E%E9%9B%B6%E9%96%8B%E5%A7%8B/TensorFlow%20%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TensorFlow 基礎介紹
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../TensorFlow%20101%20%E5%BE%9E%E9%9B%B6%E9%96%8B%E5%A7%8B/DNN%20Regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DNN Regression
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../TensorFlow%20101%20%E5%BE%9E%E9%9B%B6%E9%96%8B%E5%A7%8B/DNN%20Classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DNN Classification
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 101
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 101
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../PyTorch%20101/1.%20PyTorch%20%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. PyTorch 基礎介紹
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目錄">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目錄
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      前言
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      重新審視 Attention + RNN
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-for-seq2seq-model" class="md-nav__link">
    <span class="md-ellipsis">
      Attention for Seq2Seq Model
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention for Seq2Seq Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn-attention-qkv" class="md-nav__link">
    <span class="md-ellipsis">
      RNN 的 Attention Ｑ、K、V 計算
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention-for-seq2seq-model_1" class="md-nav__link">
    <span class="md-ellipsis">
      實際範例 Attention for Seq2Seq Model
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-without-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      Attention without RNN
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention without RNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attention-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Attention Layer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#self-attention-without-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Attention without RNN
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Self-Attention without RNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-attention-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Attention Layer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      小結
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    <span class="md-ellipsis">
      Reference
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>5.1 Transformer — Attention Is All You Need</h1>

<h2 id="_1">前言</h2>
<p>Transformer 完全基於 Attention 注意力機制的架構。Attention 原先是被應用在 RNN，之後 Google 所提出的 Transformer 保留了原先 Attention 的優勢並移除了 RNN 的架構。Transformer 是一個蠻新的模型，最先由 2017 年被 Google 所提出一篇叫 Attention is all you need 的論文。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-1.png" /></p>
<p>Transformer 是一種 Seq2seq 的模型，他有一個 Encoder 和 Decoder 並且非常適合做機器翻譯。另外在 Transformer 中拋棄了 RNN 循環神經網路的架構。Transformer 僅保留了 Attention 機制以及全連接層網路，實驗結果並優於 RNN+Attemtion 的架構。目前最新的機器翻譯研究已經很少人用 RNN 模型了，當今業界大多使用 Transformer + Bert 模型。</p>
<ul>
<li>Transformer is a Seq2Seq model.</li>
<li>Transformer is not RNN.</li>
<li>Purely based attention and dense layers.</li>
<li>Higher accuracy than RNNs on large  datasets.</li>
</ul>
<h2 id="attention-rnn">重新審視 Attention + RNN</h2>
<p>這裡來思考一個問題。當我們把 RNN 去掉只保留 Attention，僅利用 Attention 搭建一個神經網路用來取代 RNN。那我們該怎麼做呢？接下來我們會來詳細討論，從零開始基於 Attention 搭建一個神經網路的整個流程。首先在本篇文章我們先將之前學過的 RNN + Attention 開始入手，再抽取掉 RNN 保留 Attention。然後搭建一個 Attention 與 Self-Attention 網路層。下一篇文章會再將這些概念組裝起來，搭建一個深度的 Seq2seq 模型。搭出來的模型就是當今最紅的 Transformer。</p>
<h2 id="attention-for-seq2seq-model">Attention for Seq2Seq Model</h2>
<h3 id="rnn-attention-qkv">RNN 的 Attention Ｑ、K、V 計算</h3>
<p>在<a href="https://andy6804tw.github.io/2021/05/01/rnn-to-attention/">前篇</a>文章有提到 RNN 模型的進化，最終使用 Attention 機制來改善 RNN Seq2seq 的模型。所謂的 Seq2seq 是指有一個 Encoder 和一個 Decoder。Encoder 的輸入是有 m 個時間點的輸入X<sub>1</sub>~X<sub>m</sub>，每個一個輸入都是經過編碼過後的向量。Encoder 把這些輸入的訊息壓縮到隱藏狀態向量 h 中，其最後一個狀態 h<sub>m</sub> 是看過所有的輸入後所壓縮的訊息。Decoder 所做的事情取決於你的任務是什麼，例如文字生成器。在 Decoder 中會依序產生出狀態 s，每個時間點會根據狀態生成一個文字。我們在把輸出的文字作為下一次的輸入x<sup>‘</sup>，如果有 Attention 機制的話還需要計算 context vector(c)。每當計算出一個狀態 s 就要計算一次 c。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-2.png" /></p>
<p>context vector(c) 計算方式是，首先將 Decoder 當前狀態 s<sub>j</sub> 與 Encoder 所有狀態 h<sub>1</sub>~h<sub>m</sub> 做對比並用 align() 函數計算彼此間的相關性。把算出的 𝛼<sub>ij</sub>  作為注意力的分數。 每計算一次 context vector 就要計算出 m 個分數，其表示 𝛼<sub>1j</sub> ~ 𝛼<sub>mj</sub> 。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-3.png" /></p>
<p>每一個 𝛼 對應一個狀態 h，以下我們具體的來看一下分數 𝛼 是如何被計算出來的。分數的計算是 h<sub>i</sub>  和 s<sub>j</sub> 的函數，首先我們必須計算 Q 和 K。把向量 h<sub>i</sub> 乘上一個矩陣 W<sub>K</sub> 得到 k<sub>i</sub>。 另外把向量 s<sub>j</sub> 乘上一個矩陣 W<sub>Q</sub> 得到 q<sub>j</sub>。這裡的矩陣 W<sub>K</sub> 與 W<sub>Q</sub> 是 align() 函數中可以學習的權重，必須經由訓練資料中去學習的。我們必須把 s<sub>j</sub> 這一個向量與 Encoder 中的所有 h 去計算對比。有 m 個 h 向量因此會有 m 個 k，我們可以將 k<sub>1</sub>~k<sub>m</sub> 組成一個 K 矩陣。我們可以發現圖中綠色的 k<sub>i</sub> 向量為 K 的每一行(col)。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-4.png" /></p>
<p>計算每個分數需要將 K 進行轉置並與 q<sub>j</sub> 進行矩陣相乘的運算。輸出會是一個 m 維的向量。最後再使用一個 Softmax 函數將這些輸出的數值映射到 0~1 之間，並且這 m 個數值加總必為 1。此時的 𝛼<sub>1j</sub>~ 𝛼<sub>mj</sub> 為最終 q<sub>j</sub>  所對輸入有感興趣的地方的分數。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-5.png" /></p>
<p>剛才已經將 Decoder 狀態 s<sub>j</sub> 還有 Encoder 狀態 h<sub>i</sub> 分別做線性轉換，得到一組向量 q<sub>j</sub>(Query) 與 m 個 k<sub>i</sub>(Key)。我們拿一個 q<sub>j</sub> 向量去對比所有 Key(K)，算出 m 個分數，這 m 個 𝛼 分數表示了 Query 與每一個 Key 的匹配程度。其匹配程度越高 𝛼 分數越大，同時也代表著模型需要更關注這些內容。除此之外我們還需要計算 Value，將 h<sub>i</sub> 乘上一個矩陣 W<sub>V</sub> 上 就能得到 v<sub>1</sub>~v<sub>m</sub>。這些合併起來就能用 V 表示，另外這裡的 W<sub>V</sub> 也是可以透過機器學習的。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-6.png" /></p>
<h3 id="attention-for-seq2seq-model_1">實際範例 Attention for Seq2Seq Model</h3>
<p>剛剛已經講了 Q、K、V 這三種向量 在 RNN 架構中是如何被計算出來的。我們再回過頭看一下這個例子。首先我們先把 Decoder 目前狀態 s<sub>j</sub> 乘上一個 W<sub>Q</sub> 得到 q<sub>j</sub>。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-7.png" /></p>
<p>然後把 Encoder 所有 m 個狀態 h<sub>1</sub>~h<sub>m</sub> 乘上 W<sub>K</sub> 映射到 Key 向量。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-8.png" /></p>
<p>用矩陣 K 與向量 q<sub>j</sub> 計算出 m 維的分數向量。a<sub>1j</sub>~a<sub>mj</sub> 對應每個 Encoder 的 h 向量。最後還要經過一個 Softmax() 即代表對輸入 x<sub>1</sub>~x<sub>m</sub> 所需要關注的分數。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-9.png" /></p>
<p>接下來計算 Value 向量 v<sub>i</sub>，我們拿 Encoder 第 i 個狀態向量 h<sub>i</sub> 與一個權重 W<sub>V</sub> 做一個線性轉換得到 v<sub>i</sub>。每一個 v<sub>i</sub> 對應一個隱藏狀態 h。最終我們將會得到 m 個 𝛂 與 v，並做加權平均得到一組新的 context vector(c)。c<sub>j</sub> 等於 𝛼<sub>1j</sub> 乘上 v<sub>1</sub> 一直加到 𝛼<sub>mj</sub> 乘上 v<sub>m</sub>。 這種計算分數 𝛼 和 context vector(c) 的方法就是 Transformer 用的機制。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-10.png" /></p>
<h2 id="attention-without-rnn">Attention without RNN</h2>
<p>這一個部分我們來討論捨棄 RNN 只保留 Attention 的 Transformer，並得到一個 Attention 與 Self-Attention Layer。</p>
<h3 id="attention-layer">Attention Layer</h3>
<p>首先我們先設計一個 Attention Layer 用於 Seq2seq 模型，一樣包含一個 Encoder 與一個 Decoder。Encoder 的輸入向量是 x<sub>1</sub>~x<sub>m</sub>。Decoder 的輸入是 x’1~x’t。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-11.png" /></p>
<p>這裡我們捨去 RNN 只用 Attention。首先拿 Encoder 的輸入 x<sub>1</sub>~x<sub>m</sub> 來計算 Key 與 Value 向量。於是 x<sub>1</sub> 就被映射成 k<sub>1</sub> 與 v<sub>1</sub>，x2 就被映射成 k<sub>2</sub> 與 v<sub>2</sub>，依此類推我們就得到 m 組的 k 和 v 向量。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-12.png" /></p>
<p>然後把 Decoder 的輸入 x’<sub>1</sub>~x’<sub>t</sub> 做一個線性轉換乘上 Wq 得到 Query。若 Decoder 有 t 個輸入向量，則將會有 t 個 query q<sub>1</sub>~q<sub>t</sub>。注意一下目前為止總共出現了三個 W 矩陣，分別為 Encoder 中的 W<sub>K</sub> 和 W<sub>V</sub> 與 Decoder 中的 W<sub>Q</sub>。這些權重都是可以經由訓練資料進行學習的權重。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-13.png" /></p>
<p>現在開始計算分數𝛂，拿取 Decoder 中的第一個 q<sub>1</sub> 與所有 Encoder 中 m 個 k 向量做對比。透過 Scaled dot product 計算出每一個輸入的分數，也就是所謂的 Attention 關聯強度。我們將會得到 m 維的向量 𝛂1(𝛂<sub>11</sub>~𝛂<sub>m1</sub>)，裏面代表著每個相對應輸入的注意力程度。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-14.png" /></p>
<p>然後再計算 context vector c<sub>1</sub>，需要用到分數向量 𝜶<sub>1</sub> 與所有 m 個 value 向量進行加權和。又可以寫成 V𝜶<sub>1</sub>。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-15.png" /></p>
<p>接下來重複上述步驟可以得到所有 context vector，每一個 c 對應一個 x’。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-16.png" /></p>
<p>假設有七個輸出將會有七個 context vector，分別為 c<sub>1</sub>~c<sub>7</sub> 為最終的輸出。並且可以用 C 表示這些向量。想要計算一個向量 c<sub>j</sub> 要用到所有的 Q、K、V。所以 c<sub>2</sub> 依賴於 x’<sub>2</sub> 以及 Encoder 中所有的輸入，並透過注意力分數來取捨資訊。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-17.png" /></p>
<p>我們把 Attention layer 稱之為函數 Attn()，輸入是分別是 Encoder 的 x<sub>1</sub>~x<sub>m</sub> (X)以及 Decoder 的 x’<sub>1</sub>~x’<sub>t</sub> (X’)。除此之外 Attention layer 有三個要學習的權重矩陣 W<sub>Q</sub>、W<sub>K</sub>、W<sub>V</sub>。最後 Attention layer 的輸出是 c<sub>1</sub>~c<sub>t</sub> (C) 共 t 個向量。因此我們可以總結 Attention layer 有兩個輸入 X 與 X‘，以及一個輸出 C，每一個 c 向量對應一個 x’ 向量。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-18.png" /></p>
<h2 id="self-attention-without-rnn">Self-Attention without RNN</h2>
<p>剛才我們將 Seq2seq 模型剔除了 RNN，並採用 Attention Layer 替代。接下來要來了解 Self-Attention Layer。基本上原理完全一模一樣，我們可以使用 Self-Attention 來取代 RNN。</p>
<h3 id="self-attention-layer">Self-Attention Layer</h3>
<p>Self-Attention Layer 並非 Seq2seq 它僅有一個輸入序列，同時可以使用 Attn() 函數表示。此函數跟先前提的方法一模一樣，差別在於函數的輸入都是 X。此外輸出的序列是 c<sub>1</sub>~c<sub>m</sub> 與輸入 x 的虛列長度是一樣的都是 m，每一個 c 向量都對應一個 x 向量。但是必須注意，舉例來說 c<sub>2</sub> 並非只依賴於 x<sub>2</sub>，他是依賴於 x<sub>1</sub>~x<sub>m</sub> 也就是每個輸入都會考慮過才算出 c<sub>2</sub>。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-19.png" /></p>
<p>Self-Attention Layer 的原理跟 Attention Layer 完全一模一樣。只差別於輸入不同，在 Self-Attention 中僅有一個輸入序列 x<sub>1</sub>~x<sub>m</sub>。第一步是做三種轉換將 x<sub>i</sub> 映射到 q<sub>i</sub>、k<sub>i</sub>、v<sub>i</sub> 並得到三個向量。權重矩陣依然是 W<sub>Q</sub>、W<sub>K</sub>、W<sub>V</sub> 對輸入 x 做線性轉換。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-20.png" /></p>
<p>每個 x 輸入都做了線性轉換後會得到 q、k、v 三個向量。接下來再計算分數向量 𝛼 ，公式還是一樣的。我們將矩陣 K(k<sub>1</sub>~k<sub>m</sub>) 轉置乘上 q<sub>j</sub> 向量然後做 Softmax 得到 m 維向量 𝛼<sub>j</sub>。我們可以從圖中的例子看到 𝛼<sub>1</sub> 依賴於 q<sub>1</sub> 以及所有 k 向量 k<sub>1</sub>~k<sub>m</sub>。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-21.png" /></p>
<p>依此類推 𝛼<sub>2</sub> 依賴於 q<sub>2</sub> 以及所有 k 向量 k<sub>1</sub>~k<sub>m</sub>。用同樣的公式計算出所有分數向量 𝛼。總共有 m 個 𝛼 向量，此外每個分數向量都是 m 維的。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-22.png" /></p>
<p>現在可以開始計算 context vector。 c<sub>1</sub> 是所有 m  個 v 向量與 𝛼 的加權和。看以下這張圖，c<sub>1</sub> 依賴於分數向量 𝛼<sub>1</sub>，以及所有 m 個 v 向量 v<sub>1</sub>~v<sub>m</sub>。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-23.png" /></p>
<p>計算同樣的步驟算出 m 個 c 向量得到 c<sub>1</sub>~c<sub>m</sub>。這 m 個 c 向量就是 Self-Attention Layer 的輸出。其中第 j 個輸出 c<sub>j</sub> 是依賴於矩陣 V、K 以及向量 q<sub>j</sub>。
因為所有的 c<sub>j</sub> 依賴於所有的 K 與 V，所以 c<sub>j</sub> 依賴於所有 m 個 x 向量 x<sub>1</sub>~x<sub>m</sub>。下圖中每個輸入 x<sub>i</sub> 位置上都對應一個輸出 c<sub>i</sub>，每個 c<sub>i</sub> 並非只關注自己的 x<sub>i</sub> 而是依賴於所有的 x。只要改變任何一個 x 所有的 c<sub>i</sub> 都會發生變化。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-24.png" /></p>
<p>我們已經學習了 Self-Attention Layer 的運作機制。輸入是一個序列 x<sub>1</sub>~x<sub>m</sub>，這一個網路曾將有三組權重矩陣分別有 W<sub>Q</sub>、W<sub>K</sub>、W<sub>V</sub>。這三個矩陣能把每個 x 映射到 q、k、v 三個向量。其每一個輸出也是一個序列 c<sub>1</sub>~c<sub>m</sub> 共有 m 個向量，每一個 x 位置上都有對應的 c。
Attention 與 Self-Attention 都用 Attn() 這個函數來表示，此函數有兩個輸入矩陣。Attention Layer 的輸入是 X 與 X’ 而 Self-Attention 的輸入是兩個相同的 X。</p>
<p><img alt="" src="https://andy6804tw.github.io/images/posts/AI/2021/img1100727-25.png" /></p>
<h2 id="_2">小結</h2>
<p>到目前為止已經說明了 Attention 與 Self-Attention Layer，最後做一個小結。Attention 的想法最初在 2015 年由 Bengio 實驗室所發表的論文中。此篇論文使用 Attention 改進 Seq2seq 模型，後來大家發現 Attention 並不局限於 Seq2seq 模型，而是可以使用在所有的 RNN 上。如果僅有一個 RNN 網路，那麼 Attention 就稱為 Self-Attention。Self-Attention 這篇論文於 2016 年被發表，再後來 Google 於 2017 年發表的 Attention Is All You Need 表示根本不需要使用到 RNN。直接單獨使用 Attention 效果會更好。另外此篇論文中提出了 Transformer 模型架構，也就是下篇文章將提到的部分。</p>
<blockquote>
<p>本篇文章內容來至於線上課程 <a href="https://github.com/wangshusen/DeepLearning">CS583: Deep Learning</a></p>
</blockquote>
<h2 id="reference">Reference</h2>
<p>[1] Bahdanau, Cho, &amp; Bengio. Neural machine translation by jointly learning to align and  translate. In ICLR, 2015.</p>
<p>[2] Cheng, Dong, &amp; Lapata. Long Short-Term Memory-Networks for Machine Reading. In
EMNLP, 2016.</p>
<p>[3] Vaswani et al. Attention Is All You Need. In NIPS, 2017.</p>
<p><a href="https://www.youtube.com/watch?v=aButdUV0dxI">Transformer模型(1/2): 剝離RNN，保留Attention</a></p>
<p><a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/10_Transformer_1.pdf">簡報</a></p>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright © 2023 - 2024 10程式中
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "\u5df2\u8907\u88fd", "clipboard.copy": "\u8907\u88fd", "search.result.more.one": "\u6b64\u9801\u5c1a\u6709 1 \u500b\u7b26\u5408\u7684\u9805\u76ee", "search.result.more.other": "\u6b64\u9801\u5c1a\u6709 # \u500b\u7b26\u5408\u7684\u9805\u76ee", "search.result.none": "\u6c92\u6709\u7b26\u5408\u7684\u9805\u76ee", "search.result.one": "\u627e\u5230 1 \u500b\u7b26\u5408\u7684\u9805\u76ee", "search.result.other": "\u627e\u5230 # \u500b\u7b26\u5408\u7684\u9805\u76ee", "search.result.placeholder": "\u6253\u5b57\u9032\u884c\u641c\u5c0b", "search.result.term.missing": "\u7f3a\u5c11\u5b57\u8a5e", "select.version": "\u9078\u64c7\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="../../javascripts/extra.js"></script>
      
        <script src="../../javascripts/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>